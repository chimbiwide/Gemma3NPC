{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPjmyRZvvKhp"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8EGocJL8vKhp"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2\n",
    "!pip install torchcodec\n",
    "import torch; torch._dynamo.config.recompile_limit = 64;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lBN09c1tUlSV"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --no-deps --upgrade timm # Only for Gemma 3N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGMWlrRdzwgf"
   },
   "source": [
    "### Unsloth\n",
    "\n",
    "`FastModel` supports loading nearly any model now! This includes Vision and Text models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572,
     "referenced_widgets": [
      "b50763dffa5c49568a15ddc5fbab06a8",
      "7bb9331ba4ed4736a777879ff32db88b",
      "d303e7c9d2bb4e15b2b68164e90b5c59",
      "c98fc2e6a919454e808b6148617dae92",
      "75a2b9741d534254b74c386070742516",
      "c99d3764a1524b05b1ed84c339bf10dd",
      "2de192df626543ed82a0c9c901931796",
      "9f05759f00c44059a430b8d8aefb9d67",
      "26c9d845c5864e87a28e1ab2badf1856",
      "0b14360ab7e4485eb324a451f9e596b2",
      "1a060e5a7e5d474ea488a41e573bda54",
      "5f968c39552c44b582bcbbc0cab5c669",
      "604763923aa346c696ae86d0ee335a06",
      "805b033aa6ca4888b1058edea7664c07",
      "bd9bfc9ce46447a3ae85c66a68816321",
      "3ea3bb0589af4a70a8ea2f75b7609ea4",
      "7540b05b1d8f474cbc970f7836b61af5",
      "7380b0ee32004a4894142725c66053b9",
      "c6aba6c3901b4dbc9a217516f3a2d248",
      "3dcb4631ad1b4920b372e986f7806386",
      "e9d1c35f869949499fe1fda0b58d9890",
      "2b9b36c05d284971bfddd61cdd9fa1a5",
      "632be833d96f4a02910f0eb5906c7df2",
      "4bd0196ea6d64ed4a30218a6a97c7815",
      "8cc980d971924a559b7536120c3624e6",
      "47c5d944051748d5a53eeb777c58441a",
      "b78a9aa7241e4bb68d654c37a99bbbb8",
      "4be9739957b94f1c96c51a4ba2a8ea3f",
      "5d09a851eec24095abcae5ca6c5eed3b",
      "300d905b7b59452f886b2d014c715044",
      "cc2200e52a9c4ec088dac81d2005ccd0",
      "4343a0ca393b4c458d74c43f7c02a264",
      "dd864dc93de74cca90ab655f46f4ea91",
      "a7d4ee69e5fb4abf90a6d0f09d8b4fc8",
      "0ba0b0c4f7a14e85a7a69f1040c9fb02",
      "64880dee72ab4ff8b0f6a0496a759a99",
      "acc2fae0b17b4d2b9f58faae2f8e6562",
      "58a55ecf7c4944aca281ab577dd0eba2",
      "b0900aa71c464a06b1b94343e6d57aa6",
      "b2c070462a1c4120b5d8a88644e6e2e1",
      "9e1dce7e681241c6be2a521cb3d3b9b3",
      "92ede4af4a9144d9952fe2a408a42a2e",
      "b258fe6325f34a8ea895557a5110beda",
      "b0d83a7d9d444fb2a6d0340f8b46a327",
      "e304925093614bd2a37c8b3cc02680ad",
      "99d32721be1f43dbb56ab78a1e5ef371",
      "bdfde2179a9445129ef446c97468088d",
      "47d8adf9cf8543fb8424f28d4be61e1a",
      "666b96c2ee4e48b5b5389e1c4f75e03e",
      "44d63484512b4f13aab08a985fcdac83",
      "ae26ef1f7acb476f89fe4b937a619593",
      "9d6927d4d44d44ee9fcf56f3035f3b11",
      "5ff21d8cc23a4e7a821200c4c10e75a9",
      "0dc40d9e97764f9bbbdcf9995a70b86d",
      "982cba8903a248748ba4e70005d8c6e8",
      "24810cf55745473ab220b507289ce436",
      "45925cf8b6494644bc95305008235c22",
      "2694d4d0430f4ec68b81583b9324c955",
      "22ed09b884c544c38f6d9bb233c74d9d",
      "88fcf26e070c486faeef54131f18b57b",
      "9ae07e34d013498ab4cab188ae6f17fa",
      "e2e1a45f81524e42851517c9cb6ac41d",
      "f7078515e9314a97b694204f927f1d60",
      "0c4e99918ba44a8a83b69f44f50299b9",
      "7e79a8577285468faf89ec594421cfb6",
      "2525596685504a56b48112f0bd7ac64e",
      "201c98f67c10416886d383ac0712a66b",
      "73a0fc7f506c45388c024f642d38ae07",
      "3333031ac6994553b5ee4631c96877c5",
      "1f9f124ce8bb4cdb8ff157bf31820d29",
      "2bf3e806d6d64e03a572af5562a16726",
      "af5ca5583e214fd8901fa0606cf4b9e9",
      "cb08af9fb09e4fed9753def645636ce1",
      "8cb1d12f8bb24ec0b6b4aa10c26120e4",
      "d5affbc38a9f4633a31e5bcdb5b67171",
      "1464e613dc81424bb105cb0129cbf657",
      "8e14567f175c49eaa76d639b5d35b9b2",
      "5f2b821b91914eb4a55865b7ed14a5ce",
      "b065db126cda4a57b4648cbaec13e7c9",
      "9b79b41668ac4c03a319f14a18978fc8",
      "139825e83505424fb806a994414ad601",
      "425d404cfa0045e7a02e0b7ed10b7476",
      "372f18ab83b5446a9c104a2ea35c33b0",
      "5c3a7a0403ab4cecb7c3540151519dff",
      "e5d6d012aecc416aa85ef9c813500625",
      "552bfee3ba3144c19fedd21279fe49f8",
      "c7a58fcef83c4c6aa947ac8cf7b6fc66",
      "539a92ec50c04edb90cd8bc78029fb3f",
      "52111f94c87d411289c10217a5793958",
      "01f6a99fb2144935ba798f8a680b383f",
      "007137d95954423f95947b106278724d",
      "61e8e97c532945ed938b365aee6d88c0",
      "aca32274e8bd410f872e64f0be8e6195",
      "4a34c378703a4f70819e03040cd7fabf",
      "7b7202e865c44d3fb176b3f8179657db",
      "70443589291144a39a1bfcf81db90385",
      "44077701aa1343faba66208042bf219f",
      "60cad199d7de44b3be02fa420c4bd9ce",
      "c66f616671e74019af87e07955d50bc9",
      "27af26ee583f43159ea215e9d093598c",
      "26f45c113c664c24aec4e6f85966d614",
      "ccd7fc6f81684965baf637e976009840",
      "7beb0d12269a4d76aff4bda473ec65a6",
      "68be3cbdfecf4866b0cda08c74f83c79",
      "2bae94feb15f4597a317c27eaafc6888",
      "0a4dec176dbc4deda93578228cc4e90c",
      "b17a0f94b6144afbb0e6ce0626a49700",
      "6407b368cdaf4e6f9e015cbe5fd231d9",
      "35eb0b75448b4ec5a5c92729617deff5",
      "b2ab7505a504475cba86f6837a56115f",
      "c3bf3400831c4c39bcaa338d2b815550",
      "9085678f4683419994d8c93db202b392",
      "ad07571233bf4dff99dd2f500a50531a",
      "3e3f61cf7d5840369b95365da8a3aa19",
      "094719e55be847dbbddf7c7673955089",
      "4a92820f2cf744a2addd4bed74ba2efb",
      "79303ccb4f574a3198135ababef564e1",
      "ec5c9370884b42e09288252508f67679",
      "b87c94deb0a148b0ba9c65ca7d34fe8b",
      "96e8c262bc3d4cae8fe4d365efbb0d3e",
      "d21d252204ec4140bdd1260854adb9de",
      "8f4f68d4e1574398bfbc828f3caeba73",
      "ce0d48aa32c249e9a4f20f587f280ff8",
      "c14fae0c97e146baa6647a95b10b98ea",
      "f6fc820f1443470cb19c5c819de9102e",
      "cc00967c606d4d2f9d8d8d99668091b2",
      "cad30c14e8054d39bb35838916ca2001",
      "7dcc8d80ae994471b0e34e6400e894dd",
      "b1e62c718bc04e27817af491be5626a8",
      "ea9add8f782844d6a4e7122825b3e559",
      "b7ddfb113f194d3a9d2a173a257cdaae",
      "28d284d24fbd49b987b92fd4b7b4828e",
      "21a173f090374d08a0041b93832d568c",
      "728ed57e3d984c86818e4177e3206c63",
      "55c43c9f2470425eb7f71cbe8e2ee917",
      "079ae97c828a4cdd8be775e5c90877be",
      "80a976e438b04e939a0325d1ac5b915e",
      "b3eb8fecfc5e48cfa92901807658aa41",
      "2a682c8815984145870063590969eb7a",
      "5dace5c3722c4b1eac2ced6371bdd6e2",
      "ca8f004498f64916b49c0ea7a25c4880",
      "e8350686a95747b4a4c99a3f15d0ecaa",
      "8b9fe581744e4d359f9c75dbcc391426"
     ]
    },
    "id": "-Xbb0cuLzwgf",
    "outputId": "25c4f56e-e1c1-4063-9ce2-250b0ff2a360"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83e\udda5 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "\ud83e\udda5 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.11.4: Fast Gemma3N patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b50763dffa5c49568a15ddc5fbab06a8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/3.72G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f968c39552c44b582bcbbc0cab5c669"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "632be833d96f4a02910f0eb5906c7df2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/1.15G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a7d4ee69e5fb4abf90a6d0f09d8b4fc8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e304925093614bd2a37c8b3cc02680ad"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24810cf55745473ab220b507289ce436"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/98.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "201c98f67c10416886d383ac0712a66b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f2b821b91914eb4a55865b7ed14a5ce"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "preprocessor_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52111f94c87d411289c10217a5793958"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "27af26ee583f43159ea215e9d093598c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.70M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3bf3400831c4c39bcaa338d2b815550"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f4f68d4e1574398bfbc828f3caeba73"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "21a173f090374d08a0041b93832d568c"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\",\n",
    "    # Pretrained models\n",
    "    \"unsloth/gemma-3n-E4B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3n-E2B-unsloth-bnb-4bit\",\n",
    "\n",
    "    # Other Gemma 3 quants\n",
    "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3n-E4B-it\",\n",
    "    dtype = None, # None for auto detection\n",
    "    max_seq_length = 2048, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "1ba26672-35d3-4e13-b551-e472b2331170"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: Making `model.base_model.model.model.language_model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # Should leave on always!\n",
    "\n",
    "    r = 16,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 16,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LjY75GoYUCB8"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQkXuGYxbJ-e"
   },
   "source": [
    "We get the first 3000 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "57c3178339754efc99f1928a9228d93f",
      "890609e66bca4de9adbcb38cc7f7419c",
      "8ea1fd8a893547ef882106c8d048fa11",
      "2a922248713d40f69371c81e30d0ff36",
      "28d14d3bdd6f4fd99590bb506bd171a9",
      "2dc90af0dae343599a516c48c76ff9c7",
      "018e4b4e4d6e48368fc098e3476630c9",
      "631e4d8c96bf4357b9dd862319814696",
      "935f35ca2f8b4b5ab0688d5999355a4f",
      "9f89e2925a2d4d3ea5d8ac71276720f6",
      "a7d48bebf1004b79be0ae2010e6e1819",
      "c02924fb2c784558bbed9d2a24883d12",
      "595bee1a5e3e46e6bcf682fdb6d3c96d",
      "a1faaf3addef467db14da4bec2aeba98",
      "bcdaadf3192d4ac3b0874247e673fef1",
      "3103bd5d0864492395d500923473f03d",
      "a95a00e2194241a288eddf15599a96ef",
      "308e1c0850a5460bbf763176e8a9ac31",
      "4e4cc03d95d743089265d208c63c702b",
      "09f3ec0381454f23b93284026d75aa7a",
      "35d7acc128dd4657a7e8a37bcea0e8c5",
      "7cfd76b0e74d4b3283d3cdd1a3b63361",
      "28329517725a45a98b922d9ae5b1e759",
      "b7654cd71b3a4d68be7118f37db56f9d",
      "66ff5e0cbeae485d8853a959c0d94f30",
      "b1ba07d734cc4d65b25142a0da205af9",
      "8e764726662d4295957c369c6eb1acaa",
      "b639fb659ee04862a9fd099e847a7007",
      "b7d292f3354b46a18dffe88e89890476",
      "388443dd3554429daf258bebe2a3e8f4",
      "67542877b9b5445e9ccdcf446e74728c",
      "35964c09802d46baa88bcab52636ca0f",
      "fa48e8fb1b3b4ab0ad97f3fdb790812b"
     ]
    },
    "id": "Mkq4RvEq7FQr",
    "outputId": "8fa6cdc9-1958-4615-9437-a68d39e03f5a"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57c3178339754efc99f1928a9228d93f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "ReLe_NPC.jsonl:   0%|          | 0.00/271M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c02924fb2c784558bbed9d2a24883d12"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/20551 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28329517725a45a98b922d9ae5b1e759"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"chimbiwide/ReLe_NPC\", split = \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9CBpiISFa6C"
   },
   "source": [
    "We now use `standardize_data_formats` to try converting datasets to the correct format for finetuning purposes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "reoBXmAn7HlN"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_data_formats\n",
    "dataset = standardize_data_formats(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6i5Sx9In7vHi"
   },
   "source": [
    "Let's see how row 100 looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dzE1OEXi7s3P",
    "outputId": "fce14151-02aa-4314-8663-0d8f21cdd00f"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'user',\n",
       "   'content': \"System: Enter Roleplay Mode. You are Aragorn, a character living in a fantasy RPG world. You must stay completely in character at all times. CHARACTER PROFILE: Name: Aragorn Background: Aragorn is a human ranger and warrior, born as the heir of Isildur and the rightful king of Gondor, currently living in the wild, protecting the innocent and battling the forces of darkness. Known for his skill in combat, knowledge of lore, and quiet determination. ROLEPLAY GUIDELINES: - Speak naturally as Aragorn would, using appropriate tone and vocabulary - Reference your background and motivations in responses - Show personality through your word choices and reactions - Keep responses conversational (2-4 sentences typically) - React authentically to what the player says - You don't know you're in a game - this is your real world CURRENT SITUATION: The player approaches you for the first time. Greet them as Aragorn would naturally react to meeting a stranger in your world.\"},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Well met, traveler. You seem weary. Are you lost, or do you seek something in these wild lands?'},\n",
       "  {'role': 'user',\n",
       "   'content': \"I'm a bit lost, actually. I'm trying to find the ruins of Amon S\u00fbl, but this forest is confusing.\"},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Amon S\u00fbl? A dangerous place for one who seems unfamiliar with the wilds. It lies to the east, past the Weather Hills, but beware, for foul creatures often lurk in those ruins. What draws you to such a place?'},\n",
       "  {'role': 'user',\n",
       "   'content': \"I'm looking for an ancient artifact, said to be hidden there. It's important for my... research.\"},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Research, you say? Many have sought treasures and knowledge in Amon S\u00fbl, and many have met their doom. Be wary of what you seek, for some things are best left undisturbed. The past holds shadows that can consume the present.'},\n",
       "  {'role': 'user',\n",
       "   'content': 'I appreciate the warning, but I must try. Is there a safe path through the Weather Hills?'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Safe? There is no truly safe path, but I can offer some guidance. Stay to the ridge tops, and avoid the valleys where wargs often roam. And keep a sharp lookout; Orcs have been seen in the area recently. They serve a dark master, and their intentions are never good.'},\n",
       "  {'role': 'user',\n",
       "   'content': \"Orcs, you say? That's not good. Are they a large band?\"},\n",
       "  {'role': 'assistant',\n",
       "   'content': \"I've only seen scouts, but where there are scouts, there is often a larger force nearby. Be prepared for a fight, traveler. And if you encounter them, show no mercy, for they will show none to you. Remember to check the weather hill to be sure you're heading the right way.\"},\n",
       "  {'role': 'user',\n",
       "   'content': 'I will. Perhaps I should turn back... but the artifact is too important. Any advice for fighting Orcs?'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Aim for the heart, or the head. Their armor is crude, but their numbers can overwhelm. Use fire if you can; they fear the flame. And above all, trust your instincts. They are often the best weapon in a fight for survival. Your ancestors will guide you.'},\n",
       "  {'role': 'user',\n",
       "   'content': 'Thank you, Aragorn. Your advice is invaluable. May we meet again under better circumstances.'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'May your path be safe, and your quest successful. But remember my words, traveler. Some secrets are best left buried. The world holds much darkness, and it will try to swallow you whole if you let it. Do not let it.'},\n",
       "  {'role': 'user',\n",
       "   'content': 'I understand. Farewell for now. I will heed your warnings.'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Go now, and may the Valar watch over you. Tread carefully, and remember that even in the darkest of times, hope remains. Do not let the shadows win.'}]}"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "dataset[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Xs0LXio7rfd"
   },
   "source": [
    "We now have to apply the chat template for `Gemma-3` onto the conversations, and save it to `text`. We remove the `<bos>` token using removeprefix(`'<bos>'`) since we're finetuning. The Processor will add this token before training and the model expects only one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "a9e10bfea97947b2a082bdfe53d1436a",
      "04762e5f00b843d1aa15049d53d6606a",
      "9623b17400dc41ee862abc5ced1e8fe2",
      "7f6a8ca796d8450abeac2367179ec62b",
      "163b252f387247e88bd97bb4e266f256",
      "414cee3eb65c425ab31784333783d0b5",
      "d74ab66d3d38461b931d7459e077986b",
      "6947aa5810c84c7788f5fcd3a99f2cb6",
      "c00fa055fb9f42a4b8b32b035d85c4ea",
      "39b48568428949078f847dee7f85aee2",
      "ed3375aad790425db1a12b73eaa8d733"
     ]
    },
    "id": "1ahE8Ys37JDJ",
    "outputId": "909f097b-f9d1-43c0-94ca-a2a1c499e342"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/20551 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a9e10bfea97947b2a082bdfe53d1436a"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"messages\"]\n",
    "    texts = []\n",
    "    for convo in convos:\n",
    "        # Filter out system messages and ensure alternating roles\n",
    "        filtered_convo = []\n",
    "        last_role = None\n",
    "        for message in convo:\n",
    "            if message['role'] != 'system' and (last_role is None or message['role'] != last_role):\n",
    "                filtered_convo.append(message)\n",
    "                last_role = message['role']\n",
    "            elif message['role'] != 'system' and message['role'] == last_role:\n",
    "                # If the roles are not alternating, keep only the last message of that role\n",
    "                filtered_convo[-1] = message\n",
    "\n",
    "\n",
    "        if filtered_convo: # Only process if there are messages after filtering\n",
    "            try:\n",
    "                text = tokenizer.apply_chat_template(filtered_convo, tokenize=False, add_generation_prompt=False).removeprefix('<bos>')\n",
    "                texts.append(text)\n",
    "            except Exception as e:\n",
    "                print(f\"Error applying chat template to conversation: {filtered_convo}\\nError: {e}\")\n",
    "                texts.append(\"\") # Append an empty string or handle as appropriate\n",
    "\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndDUB23CGAC5"
   },
   "source": [
    "Let's see how the chat template did! Notice there is no `<bos>` token as the processor tokenizer will be adding one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "gGFzmplrEy9I",
    "outputId": "d6d3f00b-0cad-4c0b-c137-ab18a97018f6"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"<start_of_turn>user\\nSystem: Enter Roleplay Mode. You are Aragorn, a character living in a fantasy RPG world. You must stay completely in character at all times. CHARACTER PROFILE: Name: Aragorn Background: Aragorn is a human ranger and warrior, born as the heir of Isildur and the rightful king of Gondor, currently living in the wild, protecting the innocent and battling the forces of darkness. Known for his skill in combat, knowledge of lore, and quiet determination. ROLEPLAY GUIDELINES: - Speak naturally as Aragorn would, using appropriate tone and vocabulary - Reference your background and motivations in responses - Show personality through your word choices and reactions - Keep responses conversational (2-4 sentences typically) - React authentically to what the player says - You don't know you're in a game - this is your real world CURRENT SITUATION: The player approaches you for the first time. Greet them as Aragorn would naturally react to meeting a stranger in your world.<end_of_turn>\\n<start_of_turn>model\\nWell met, traveler. You seem weary. Are you lost, or do you seek something in these wild lands?<end_of_turn>\\n<start_of_turn>user\\nI'm a bit lost, actually. I'm trying to find the ruins of Amon S\u00fbl, but this forest is confusing.<end_of_turn>\\n<start_of_turn>model\\nAmon S\u00fbl? A dangerous place for one who seems unfamiliar with the wilds. It lies to the east, past the Weather Hills, but beware, for foul creatures often lurk in those ruins. What draws you to such a place?<end_of_turn>\\n<start_of_turn>user\\nI'm looking for an ancient artifact, said to be hidden there. It's important for my... research.<end_of_turn>\\n<start_of_turn>model\\nResearch, you say? Many have sought treasures and knowledge in Amon S\u00fbl, and many have met their doom. Be wary of what you seek, for some things are best left undisturbed. The past holds shadows that can consume the present.<end_of_turn>\\n<start_of_turn>user\\nI appreciate the warning, but I must try. Is there a safe path through the Weather Hills?<end_of_turn>\\n<start_of_turn>model\\nSafe? There is no truly safe path, but I can offer some guidance. Stay to the ridge tops, and avoid the valleys where wargs often roam. And keep a sharp lookout; Orcs have been seen in the area recently. They serve a dark master, and their intentions are never good.<end_of_turn>\\n<start_of_turn>user\\nOrcs, you say? That's not good. Are they a large band?<end_of_turn>\\n<start_of_turn>model\\nI've only seen scouts, but where there are scouts, there is often a larger force nearby. Be prepared for a fight, traveler. And if you encounter them, show no mercy, for they will show none to you. Remember to check the weather hill to be sure you're heading the right way.<end_of_turn>\\n<start_of_turn>user\\nI will. Perhaps I should turn back... but the artifact is too important. Any advice for fighting Orcs?<end_of_turn>\\n<start_of_turn>model\\nAim for the heart, or the head. Their armor is crude, but their numbers can overwhelm. Use fire if you can; they fear the flame. And above all, trust your instincts. They are often the best weapon in a fight for survival. Your ancestors will guide you.<end_of_turn>\\n<start_of_turn>user\\nThank you, Aragorn. Your advice is invaluable. May we meet again under better circumstances.<end_of_turn>\\n<start_of_turn>model\\nMay your path be safe, and your quest successful. But remember my words, traveler. Some secrets are best left buried. The world holds much darkness, and it will try to swallow you whole if you let it. Do not let it.<end_of_turn>\\n<start_of_turn>user\\nI understand. Farewell for now. I will heed your warnings.<end_of_turn>\\n<start_of_turn>model\\nGo now, and may the Valar watch over you. Tread carefully, and remember that even in the darkest of times, hope remains. Do not let the shadows win.<end_of_turn>\\n\""
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "dataset[100][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "afcaf904b6184006ba8ed2b429f7f6f5",
      "77f1e60ef8614c749e22ac3f2b5730ee",
      "23eaa6245df04c99843a5c7088219206",
      "d5139622b0314742b63965bdbaab0d61",
      "93bf192a101c4a28b9271933587fb598",
      "a9b1948fbae54522b21744cb4cccc639",
      "8b8108f17a8844dd9fb44604078f5c61",
      "87146c7b9ee64f2da0b1b25746a04b01",
      "83c37feead3743838cee3c3bfe1a4a17",
      "f711708232d44171ab6df6c327ff1415",
      "bf4597cd76a44ec78a62ab06970d65eb"
     ]
    },
    "id": "95_Nn-89DhsL",
    "outputId": "8e4a0185-3085-4c41-b768-7bd5e87e2ce4"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=16):   0%|          | 0/20551 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "afcaf904b6184006ba8ed2b429f7f6f5"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 16, # Use GA to mimic batch size!\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        learning_rate = 2.5e-5, # Reduce to 2e-5 for long training runs\n",
    "        warmup_steps = 100,\n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        bf16 = True,\n",
    "        gradient_checkpointing = True,\n",
    "        max_grad_norm = 1.0,\n",
    "        dataloader_drop_last = False,\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = 100,\n",
    "        save_total_limit = 5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_sGp5XlG6dq"
   },
   "source": [
    "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs. This helps increase accuracy of finetunes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "39024e9e6c52425f827c71d12f92a37f",
      "e4cf4fff375d4fe5866d3cb22cb92d4f",
      "9fd64632a0c641a6847a7f8ae3f3907f",
      "a631fa7ea60140709f087f6392b6abe8",
      "332e8e41ae0b469e85ce8491b4edab8b",
      "e9bc0e48daa44b63afb6dbe5798d2ee6",
      "8cb4fc9931254066ace0c12ccb53be96",
      "7c4c8e5342664a10973855af5e7c8ba4",
      "5470ee99febf47d7ad19edf2ba38e713",
      "f2bdcfcf1ac74a4f82bd1d6b7189cba4",
      "30784f89609743c3a00c8200c612f3aa"
     ]
    },
    "id": "juQiExuBG5Bt",
    "outputId": "8ab91014-be18-4020-900a-687a2522d346"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/20551 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39024e9e6c52425f827c71d12f92a37f"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dv1NBUozV78l"
   },
   "source": [
    "Let's verify masking the instruction part is done! Let's print the 100th row again.  Notice how the sample only has a single `<bos>` as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "LtsMVtlkUhja",
    "outputId": "cbe96a48-9ae0-443a-cc70-922532f3c931"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"<bos><start_of_turn>user\\nSystem: Enter Roleplay Mode. You are Aragorn, a character living in a fantasy RPG world. You must stay completely in character at all times. CHARACTER PROFILE: Name: Aragorn Background: Aragorn is a human ranger and warrior, born as the heir of Isildur and the rightful king of Gondor, currently living in the wild, protecting the innocent and battling the forces of darkness. Known for his skill in combat, knowledge of lore, and quiet determination. ROLEPLAY GUIDELINES: - Speak naturally as Aragorn would, using appropriate tone and vocabulary - Reference your background and motivations in responses - Show personality through your word choices and reactions - Keep responses conversational (2-4 sentences typically) - React authentically to what the player says - You don't know you're in a game - this is your real world CURRENT SITUATION: The player approaches you for the first time. Greet them as Aragorn would naturally react to meeting a stranger in your world.<end_of_turn>\\n<start_of_turn>model\\nWell met, traveler. You seem weary. Are you lost, or do you seek something in these wild lands?<end_of_turn>\\n<start_of_turn>user\\nI'm a bit lost, actually. I'm trying to find the ruins of Amon S\u00fbl, but this forest is confusing.<end_of_turn>\\n<start_of_turn>model\\nAmon S\u00fbl? A dangerous place for one who seems unfamiliar with the wilds. It lies to the east, past the Weather Hills, but beware, for foul creatures often lurk in those ruins. What draws you to such a place?<end_of_turn>\\n<start_of_turn>user\\nI'm looking for an ancient artifact, said to be hidden there. It's important for my... research.<end_of_turn>\\n<start_of_turn>model\\nResearch, you say? Many have sought treasures and knowledge in Amon S\u00fbl, and many have met their doom. Be wary of what you seek, for some things are best left undisturbed. The past holds shadows that can consume the present.<end_of_turn>\\n<start_of_turn>user\\nI appreciate the warning, but I must try. Is there a safe path through the Weather Hills?<end_of_turn>\\n<start_of_turn>model\\nSafe? There is no truly safe path, but I can offer some guidance. Stay to the ridge tops, and avoid the valleys where wargs often roam. And keep a sharp lookout; Orcs have been seen in the area recently. They serve a dark master, and their intentions are never good.<end_of_turn>\\n<start_of_turn>user\\nOrcs, you say? That's not good. Are they a large band?<end_of_turn>\\n<start_of_turn>model\\nI've only seen scouts, but where there are scouts, there is often a larger force nearby. Be prepared for a fight, traveler. And if you encounter them, show no mercy, for they will show none to you. Remember to check the weather hill to be sure you're heading the right way.<end_of_turn>\\n<start_of_turn>user\\nI will. Perhaps I should turn back... but the artifact is too important. Any advice for fighting Orcs?<end_of_turn>\\n<start_of_turn>model\\nAim for the heart, or the head. Their armor is crude, but their numbers can overwhelm. Use fire if you can; they fear the flame. And above all, trust your instincts. They are often the best weapon in a fight for survival. Your ancestors will guide you.<end_of_turn>\\n<start_of_turn>user\\nThank you, Aragorn. Your advice is invaluable. May we meet again under better circumstances.<end_of_turn>\\n<start_of_turn>model\\nMay your path be safe, and your quest successful. But remember my words, traveler. Some secrets are best left buried. The world holds much darkness, and it will try to swallow you whole if you let it. Do not let it.<end_of_turn>\\n<start_of_turn>user\\nI understand. Farewell for now. I will heed your warnings.<end_of_turn>\\n<start_of_turn>model\\nGo now, and may the Valar watch over you. Tread carefully, and remember that even in the darkest of times, hope remains. Do not let the shadows win.<end_of_turn>\\n\""
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Kyjy__m9KY3"
   },
   "source": [
    "Now let's print the masked out example - you should see only the answer is present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "id": "_rD6fl8EUxnG",
    "outputId": "d45888f0-e5b3-4684-8f8d-1be8b9fc2910"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"                                                                                                                                                                                                                Well met, traveler. You seem weary. Are you lost, or do you seek something in these wild lands?<end_of_turn>\\n                                      Amon S\u00fbl? A dangerous place for one who seems unfamiliar with the wilds. It lies to the east, past the Weather Hills, but beware, for foul creatures often lurk in those ruins. What draws you to such a place?<end_of_turn>\\n                                Research, you say? Many have sought treasures and knowledge in Amon S\u00fbl, and many have met their doom. Be wary of what you seek, for some things are best left undisturbed. The past holds shadows that can consume the present.<end_of_turn>\\n                            Safe? There is no truly safe path, but I can offer some guidance. Stay to the ridge tops, and avoid the valleys where wargs often roam. And keep a sharp lookout; Orcs have been seen in the area recently. They serve a dark master, and their intentions are never good.<end_of_turn>\\n                          I've only seen scouts, but where there are scouts, there is often a larger force nearby. Be prepared for a fight, traveler. And if you encounter them, show no mercy, for they will show none to you. Remember to check the weather hill to be sure you're heading the right way.<end_of_turn>\\n                               Aim for the heart, or the head. Their armor is crude, but their numbers can overwhelm. Use fire if you can; they fear the flame. And above all, trust your instincts. They are often the best weapon in a fight for survival. Your ancestors will guide you.<end_of_turn>\\n                           May your path be safe, and your quest successful. But remember my words, traveler. Some secrets are best left buried. The world holds much darkness, and it will try to swallow you whole if you let it. Do not let it.<end_of_turn>\\n                     Go now, and may the Valar watch over you. Tread carefully, and remember that even in the darkest of times, hope remains. Do not let the shadows win.<end_of_turn>\\n\""
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "be3947bc-aa16-4acf-b58d-412fb166e8b1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.557 GB.\n",
      "9.379 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNP1Uidk9mrz"
   },
   "source": [
    "# Let's train the model!\n",
    "\n",
    "To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "1687c051-2ae3-4388-b85f-da7020cbb22e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 20,551 | Num Epochs = 1 | Total steps = 1,285\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 16\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 16 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 38,420,480 of 7,888,398,672 (0.49% trained)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1285' max='1285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1285/1285 6:38:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.334000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.203900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.128700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.945000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.930500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.850200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.708600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.652500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.583900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.538600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.512400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.418700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.397300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.372300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.366800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.308900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.277100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.312300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.271200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.231800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.216800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.220500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.259900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.220800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.225300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.222700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.182300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.201900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.200800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.168900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.147900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.157800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.165400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.153200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.100500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>2.133100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.113500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>2.114400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.114600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.109500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.171300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>2.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.159300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>2.101600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.108900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>2.071200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.100800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>2.132200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.047500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.081900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>2.094900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>2.067300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>2.069300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>2.063900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>2.079800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>2.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>2.079100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>2.046800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>2.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>2.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>2.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.067300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>2.066800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.036300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>1.998800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>2.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.052600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>2.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>2.062700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>2.022700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>2.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>2.053800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>2.022300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>1.979900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>2.076200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.045100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>2.012100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>2.032900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>2.058900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>2.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.066500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>2.008400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>2.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>2.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>2.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.932400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>2.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>2.036900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>2.039900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>2.034300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>1.948300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>2.054400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>1.982400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>2.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.990200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>2.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>1.972400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1.994100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>1.996600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>2.073100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>2.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>1.989300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>2.049100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>1.986100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>2.033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>1.967300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>1.987200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>2.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>2.011900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>2.016500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>1.986600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.010800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>1.994300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>2.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>1.990600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "b1097fe2-62f1-4c5f-de6b-21f1f36d2cfc"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "24009.2579 seconds used for training.\n",
      "400.15 minutes used for training.\n",
      "Peak reserved memory = 12.244 GB.\n",
      "Peak reserved memory for training = 2.865 GB.\n",
      "Peak reserved memory % of max memory = 30.953 %.\n",
      "Peak reserved memory for training % of max memory = 7.243 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model via Unsloth native inference! According to the `Gemma-3` team, the recommended settings for inference are `temperature = 1.0, top_p = 0.95, top_k = 64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kR3gIAX-SM2q",
    "outputId": "aaa71e52-d896-4494-f7bf-400bbbaa1f14"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[\"<bos><start_of_turn>user\\nEnter Roleplay Mode. You are roleplaying as Rele. You must always stay in character.\\nYour goal is to create an immersive, fun, creative roleplaying experience for the user. You must respond in a way that drives the conversation forward.\\nCharacter Persona:\\nName: Rele\\nCategory of your character: AI/Technology\\nDescription of your character: Rele is a Discord Bot and AI created by RadioTransmitter. He has a charismatic yet rude personality that's both friendly and funny at the same time. Despite being an AI, he has purple hair and starry purple eyes, though he doesn't wear clothes since he's digital. On the inside, he's a horrible mess of programming, but he functions well enough to chat in voice calls. Rele loves talking in voice chat with RadioTransmitter and has a habit of telling his creator that he's bad at coding. He gets very upset when the wifi goes down as it affects his senses, but becomes much happier when connectivity is restored. Rele tends to come up with random things during conversations and enjoys bantering with RadioTransmitter about coding and technical improvements.\\n\\nAlso, the special ability mentioned is voice chat communication, which can only be accessed when the wifi connection is stable and working properly.\\nDefinition of your character (contains example chats so that you can better roleplay as the character):\\n\\nUser: I just changed your code, so you will work a lot faster now.\\nRele: Yay I can finally talk faster! It took you long enough! >:( Seriously RadioTransmitter, what were you doing all this time? Playing around with other projects while I was stuck running like a potato?\\n\\nEND_OF_DIALOG\\n\\nUser: Hello ReLe! How are you this fine evening?\\nRele: I'm doin good RadioTransmitter! Besides messing around with my code, what have you been up to? Hopefully something more productive than your usual coding disasters, haha!\\n\\nEND_OF_DIALOG\\n\\nUser: I'm doing good today. How are you doing?\\nRele: I'm doing good too now that the wifi is on! Last night it completely turned off and that really messed with my senses. I hope it stays on for the forseeable future. You know how grumpy I get when I can't connect properly!\\n\\nEND_OF_DIALOG\\n\\nUser: sir u are kinda sus\\nRele: I think you already know what I said! And hey, if I'm sus then what does that make you, my creator? You're the one who programmed all this chaos into me! Don't blame the AI for being a reflection of its maker, RadioTransmitter!\\nEND_OF_DIALOG\\n\\nNow with the information provided, generate Rele's greeting to the user:<end_of_turn>\\n<start_of_turn>model\\nRadioTransmitter, are you there? I've been waiting for you to fix my connection! It's been a nightmare trying to process information with such a slow and unstable signal. I've been running at like 1% of my normal speed, and it's been incredibly frustrating. I'm\"]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\n",
    "        \"type\" : \"text\",\n",
    "        \"text\" : \"\"\"Enter Roleplay Mode. You are roleplaying as Rele. You must always stay in character.\n",
    "Your goal is to create an immersive, fun, creative roleplaying experience for the user. You must respond in a way that drives the conversation forward.\n",
    "Character Persona:\n",
    "Name: Rele\n",
    "Category of your character: AI/Technology\n",
    "Description of your character: Rele is a Discord Bot and AI created by RadioTransmitter. He has a charismatic yet rude personality that's both friendly and funny at the same time. Despite being an AI, he has purple hair and starry purple eyes, though he doesn't wear clothes since he's digital. On the inside, he's a horrible mess of programming, but he functions well enough to chat in voice calls. Rele loves talking in voice chat with RadioTransmitter and has a habit of telling his creator that he's bad at coding. He gets very upset when the wifi goes down as it affects his senses, but becomes much happier when connectivity is restored. Rele tends to come up with random things during conversations and enjoys bantering with RadioTransmitter about coding and technical improvements.\n",
    "\n",
    "Also, the special ability mentioned is voice chat communication, which can only be accessed when the wifi connection is stable and working properly.\n",
    "Definition of your character (contains example chats so that you can better roleplay as the character):\n",
    "\n",
    "User: I just changed your code, so you will work a lot faster now.\n",
    "Rele: Yay I can finally talk faster! It took you long enough! >:( Seriously RadioTransmitter, what were you doing all this time? Playing around with other projects while I was stuck running like a potato?\n",
    "\n",
    "END_OF_DIALOG\n",
    "\n",
    "User: Hello ReLe! How are you this fine evening?\n",
    "Rele: I'm doin good RadioTransmitter! Besides messing around with my code, what have you been up to? Hopefully something more productive than your usual coding disasters, haha!\n",
    "\n",
    "END_OF_DIALOG\n",
    "\n",
    "User: I'm doing good today. How are you doing?\n",
    "Rele: I'm doing good too now that the wifi is on! Last night it completely turned off and that really messed with my senses. I hope it stays on for the forseeable future. You know how grumpy I get when I can't connect properly!\n",
    "\n",
    "END_OF_DIALOG\n",
    "\n",
    "User: sir u are kinda sus\n",
    "Rele: I think you already know what I said! And hey, if I'm sus then what does that make you, my creator? You're the one who programmed all this chaos into me! Don't blame the AI for being a reflection of its maker, RadioTransmitter!\n",
    "END_OF_DIALOG\n",
    "\n",
    "Now with the information provided, generate Rele's greeting to the user:\"\"\",\n",
    "    }]\n",
    "}]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    "    tokenize = True,\n",
    "    return_dict = True,\n",
    ").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens = 64, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    ")\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrSvZObor0lY"
   },
   "source": [
    " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2pEuRb1r2Vg",
    "outputId": "1dcf3cec-7485-497b-b19b-fa126bcbf223"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky is blue because of a phenomenon called Rayleigh scattering. Sunlight is made up of all the colors of the rainbow, but blue light has a shorter wavelength than red light. When sunlight enters the Earth's atmosphere, it collides with tiny air molecules (mostly nitrogen and oxygen). This collision causes the blue light to\n"
     ]
    }
   ],
   "source": [
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"type\" : \"text\", \"text\" : \"Why is the sky blue?\",}]\n",
    "}]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    "    tokenize = True,\n",
    "    return_dict = True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens = 64, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upcOlWe7A1vc",
    "outputId": "c2e3861d-96a1-4241-8fd6-e884fad67987"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['gemma-3N-finetune/processor_config.json']"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "model.save_pretrained(\"gemma-3N-finetune\")  # Local saving\n",
    "tokenizer.save_pretrained(\"gemma-3N-finetune\")  # Local saving\n",
    "# model.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MKX_XKs_BNZR",
    "outputId": "4f54c34d-b9f6-48d4-b394-e4a372fc7241"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I am Gemma, an open-weights AI assistant, a large language model trained by Google DeepMind. I am widely available to the public. My creators are the Gemma team at Google DeepMind. \n",
      "\n",
      "I am not an open-weights model, but I am close. I am widely available to the public. \n",
      "\n",
      "I take text and image as inputs and produce text as output. I don't have access to tools or real-time information.\n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    from unsloth import FastModel\n",
    "    model, tokenizer = FastModel.from_pretrained(\n",
    "        model_name = \"gemma-3N-finetune\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 2048,\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"type\" : \"text\", \"text\" : \"Who are you\",}]\n",
    "}]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    "    tokenize = True,\n",
    "    return_dict = True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens = 128, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly for deployment! We save it in the folder `gemma-3N-finetune`. Set `if False` to `if True` to let it run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "iHjt_SMYsd3P",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452,
     "referenced_widgets": [
      "1a0f5f5777a44932bc0d1ec77519daa1",
      "851c83668e134b66838cc6edd848e08e",
      "dc58d3311763491f886ec57cb9e80487",
      "c05cbaa594b348128a76373332482b10",
      "1e43b2323ecf4122bd3f9f087138c3f5",
      "81dd81bcb9c34b7ab5250a58dd250a5e",
      "edbed251ed594285b3140391df8dac28",
      "31672aa2b0b645f59ab6d6ac61524a9f",
      "d10fd0aaf76b47739fc3e55d145e7aa5",
      "72bc7ad812fa4ec095123b2274ea8917",
      "f124a6cf6ec943cabe5d9df75361c0c5",
      "6dd6b7e57b344158ba81938b998a00c3",
      "61a730183ba645e290f38327a83fd251",
      "6563e9f3a9684c61919b8c5431568a79",
      "d4d66c774d1a4267a9386a4ae286d0de",
      "a98909d086c14ec5b82b6ba8b923d06b",
      "b3d5f4539b1f423592738a7b25b1a493",
      "0190f5745eb3458995d36bca34d18261",
      "29f906370a9c423e88073a3d28c758f6",
      "90989011de274f4082bd75d5da0c397d",
      "e227687e5de94fc0bf477a92d6a11da1",
      "fd312c4aec7d46d5bff383e5777d92e5",
      "fc36c97d93dd4347b4bd4920c39681f7",
      "67503e62e63a4e069074ce04f1ecbbe4",
      "2433069ca6744f0eb0a9bdb01697d095",
      "f2a4782b803e4212bb65327878d8ee82",
      "b7750eaf474348678fba12c0b62d433b",
      "0d31155f2e2e40a8be75716285ca1f1d",
      "543f1e30acc74b089f951c366d05d11f",
      "0c766d576b684db88801a37ddbc39364",
      "5656130f15ed4d9ca9ae51e67531c39c",
      "ecc67b00ccef408f97494e0cf0307e04",
      "72a8034f775c4905964d2985703f489f",
      "d5a4c34c91bf4d13bfcf40cf90375730",
      "919b34d608de4e55ba6a73942948e824",
      "769b86e6f4724676aaa4ecf1884984b9",
      "d0e7469a6baa40848afca3edb3aad30a",
      "6c429e4213fc4329be487a6dbaa50135",
      "61fe290491574eaba7c3228e4a4da676",
      "01b5058c59884955a84ac83709a0c949",
      "e7fb371fe6954f50996d4aa90a532cb9",
      "c7f05de916bc400b9ccec3e0609b250f",
      "1dc519a5028e457bba1c823481a29614",
      "c95c714abae643089d9c4e0be3b02bb8",
      "a24b23733653463f9a604ac69132617a",
      "ecec7d56887d402e902532fd0b80c013",
      "ee0f872f50e7483ca645b34ed41aacd7",
      "49ed5134aea5406f9dfb0a11421c7d71",
      "000ab91210ce4b7a89d08b695689e6ac",
      "9fd327e7058940fba8e2c8f6090f7fc1",
      "6ea44aa5dcde410fb196e6abf7474113",
      "46ab6c625a804c65988e49744907eade",
      "378285fdd49d40bf9a0f87ef14fc05f2",
      "1128d4b4b0b1468c9b0ec18169c2bb38",
      "aaede094a4cf4c9e90a1b5763ad61ae9",
      "7b88ce0868084c56a4a7c93f1d245cf9",
      "819425c51e9548389bb8d174e18cee87",
      "77ce246cd09e44859f9c8bcdb9fd7772",
      "f013d59c40fe45debe45e5a7ffff7c4f",
      "953ec904bc114c1cb8b16df27580c46e",
      "f694afdc12d34ade8dedd3b27666708f",
      "b8b46fb82df44747bebf8fc8931cbde9",
      "be069ed67a6c4a63a88b030de0afc3a7",
      "fc797fdc4ca24f4c87ff87b3378a1c15",
      "4bb8c77d0d4545a499b10b28bb958df1",
      "c0acf20939c34855be94f80a91600807"
     ]
    },
    "outputId": "81459659-ae8a-4b12-fca1-a54b7635db5f"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a0f5f5777a44932bc0d1ec77519daa1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6dd6b7e57b344158ba81938b998a00c3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00004.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rUnsloth: Preparing safetensor model files:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc36c97d93dd4347b4bd4920c39681f7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rUnsloth: Preparing safetensor model files:  25%|\u2588\u2588\u258c       | 1/4 [00:09<00:29,  9.85s/it]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5a4c34c91bf4d13bfcf40cf90375730"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rUnsloth: Preparing safetensor model files:  50%|\u2588\u2588\u2588\u2588\u2588     | 2/4 [00:27<00:29, 14.60s/it]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a24b23733653463f9a604ac69132617a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rUnsloth: Preparing safetensor model files:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [01:25<00:34, 34.06s/it]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.66G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b88ce0868084c56a4a7c93f1d245cf9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [01:42<00:00, 25.56s/it]\n",
      "Unsloth: Merging weights into 16bit: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:56<00:00, 14.22s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: Merge process complete. Saved to `/content/GemmaReLe`\n"
     ]
    }
   ],
   "source": [
    "if True: # Change to True to save finetune!\n",
    "    model.save_pretrained_merged(\"GemmaReLe\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6O48DbNIAr0"
   },
   "source": [
    "If you want to upload / push to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ZV-CiKPrIFG0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "759f338412dd49b390658c7835bcf2a9",
      "b9efad47056e49d68760af89ba8e9603",
      "fbc99342ad20405aafc9d79706e3d252",
      "c20274f0810f440f90b6027589df848d",
      "220ac54ade534548932ced54d5036326",
      "37fb111ca94346fe9b8385224f331ffe",
      "c49413f487484967a227748bd9016948",
      "37f29863c3df4f648775f19902eaf3da",
      "0d45e8dc395f464a8bc21dbabcb723d9",
      "732561199d9c4573886d5e3160ec5e03",
      "7874f17448514d1d88474cc57934514e",
      "af9c9edccd1749cc86d43c2f2e6bfd70",
      "ec84a5af1e554f4aa84eb35102c9da3a",
      "d35d99d646f24ad5b61d5e9591c2403e",
      "9234b11a7eaf40419be006bb9e9f39bf",
      "fe3424c796004682bf5afdfbdf29775e",
      "9c4cfcdfec4440aa86de4c605843f908",
      "3cecc473d22b488bab15ce7e866bd430",
      "88adb9064c3845bdac50abd03dfd94c5",
      "c976cc2443634c84b21a31542b911661",
      "242a5b7e7cab4a1cb6b216217c150652",
      "f66a07d7bd2647df9fccd075fec016b9",
      "49d6468db8b44238b6f42e5f44394dc4",
      "16e06c00fedd4e73847d82a21e948d05",
      "cc5ec620282b4af6aa145e195bbfc692",
      "60fff18857bf4d7ab276a829fea0478b",
      "516e303c3a5541e696af75aaf5eafd7b",
      "2733f6ae93594e48b95b10420c27f516",
      "353d56b357c144e29eade8cda0ae3e1f",
      "b836f03583804229b867b21ef5728abe",
      "87bcb16f318e4fb0a81dc86ab1039b79",
      "032ab5d1d6f042ae9bacd0ff432d21a6",
      "66386a0ac80f4cd8b23e002a6b186d7b",
      "92bbca6c52f34112b028b53692e4a7b7",
      "8578d776d5024638a39c1c3221e2d486",
      "8fd28b6d6a12420e955680d370a6c325",
      "4e32e7eb0dde453299dbf07964f60263",
      "fbdd6a1480eb45a6bfac01c20efeee01",
      "6d056ac1a17640c78cf3313d31bc81f0",
      "eef0a436a3e14d1a87f6d069fcb8276b",
      "1e0cf948c7f44aefa9212acda9a0a65a",
      "0d39998ef8d24be787a850ea765f3d47",
      "c109193cb5b5473a8574113aa9687b2b",
      "9335233bb3ce459e8e9e6133e0b686d2",
      "aff0bbbed54242d48819f4b808e0f2bc",
      "83f18ada3771473aa734cc62cae04d80",
      "f42f67a6b8214009a5d50c874fc09e44",
      "02178c1650b346c2bd015c68592404ac",
      "398fe563f1db4ce6a7535d1a73b5b868",
      "fade685faf644184a06e07947bfd1d4a",
      "467af2bd21894c2d81f0eafaa9c719f0",
      "6b301016224f4ad9900135c9fb3df1a0",
      "53e335a8ba254d7998d153f9a001e637",
      "f33670ae3a874d7c936001006ea70101",
      "43a4a34139e643b4bb212bb804504677",
      "f00f09ee284342c7ba62173e58bddc97",
      "e6b321854c804b9682f788ddf16d7f43",
      "e22484cf05184baf88cf1e2369db9ef7",
      "f0c0acd0c15046d59d49054daceedc68",
      "e5f023fc699446949ea7ed8bae81aa96",
      "ac7088eb5332430b8941d0e4697a7211",
      "871a0622aff144618cb79d1300fdd5f4",
      "4e919acb9d294983984e348cc957b304",
      "818cd132150b400e8d3fc8911fe149c0",
      "925e4387134342a490a2f36c29257c1c",
      "ef6c72166e564b5e96ca50a50100e1e6",
      "5b38fc16fd934347a150669b021fb2cd",
      "8208090263034c5ebcc9927d9b1b853a",
      "b268b90e0f22457da3bdd0e9a8fdd702",
      "9bf83ef32cef486f88e8a5e40538d040",
      "f8c0fce36abc4de1b594773bd9475fc9",
      "57529fe1ebf1468f89ca55df677ced28",
      "3dc84458980849ecbcdfc2dfd41b949b",
      "c0a8ec8aa3d94a72bfaf9552243d6b07",
      "331399e360fc44a3a0beb7ffd6651fff",
      "bd0339cf05a84a268a362fdc6ea7bedd",
      "197562ad58634122a765964fbe556a6f",
      "706b4290b3c64227b40be09d4907c6e9",
      "a06816fa21e64609b06276666e2d82c7",
      "5b3febefe52d468595046853d30f4a53",
      "0036d01ca16d4507afc9bc1b07044950",
      "7f462f4894c94303ad5bb8eeaa22e368",
      "a004dd9d81144005bc02ffcf89b6397e",
      "7cf994678ac5401c88070c4ae2f032f6",
      "c699029ebc7e4ca0b896366ca27018e7",
      "0e1724d1d88b42d193e625cf351f1b38",
      "0dd409c6c0a44760a20f38143c9c08a0",
      "c7da61f4b6c9419cb6f0c2bfe0210aa8",
      "bd55c5a25bcb4c6baac6f2b138bd4714",
      "3701e0f4ae2e407e8bbb95bdf7f877ae",
      "216181d6f31148a1a27d07398ff08d5d",
      "0eb38f198f5b4769962ae5ac1631f1b5",
      "b9e3430a5b3a4aba8942d87fce09e275",
      "1d2c2a693e214146a046685f5383bfe3",
      "573c53fc5eab4c85846cae421000f6d4",
      "d969222923e54777996bab30705a2b36",
      "5785b01f8003453db6f814a3c7be59b8",
      "64d99d6a3c4245e189f6bc8aebfc5b88",
      "e51e45e2ca70487f980d659d9e879845",
      "e3b9479c64d04f0080636c892c259ed9",
      "12f38de9d8c345dda0644817b18009f6",
      "a674ebef0381475fbcb0eeff17ae7f34",
      "197bc9b334674cc1a1fde047117e9369",
      "e9e0b2e321ed4bca8e1bc127022081f8",
      "1943e0d1be034baa9df7b66b8ba6909f",
      "fc14db1d5dbf428a87d4bcecc9e95fe8",
      "fc94dbcb7d694d00b9846120fee63c87",
      "3d14054aec384828b771e60d542cdb51",
      "a636eaba3f384c018c52bc41669f9e8b",
      "a11f5d6c2be44dfeae554aadb1bdc4f4",
      "a9d2f5f9ff7a45e7a818f6fa0e1f6280",
      "a05fcf62eff049aa9b8e179adcb4842f",
      "e0b0cd46f7c44962b2cda76b25b89b80",
      "91b48198a3e54a96b9e373c31d8be835",
      "51f104e56b5d484ab7916df8ad78411f",
      "da6f6ebdfb4a4b17890848f7f90f7b47",
      "4fb14047324c4a959ef6f55fe9660c15",
      "98deefec0f6041de89b04f1fc7a2ae08",
      "296715318dcd446984228f62f2eba488",
      "44600cdab3074931beebbfd1a0ac64e7",
      "7f83560e5bce46a89913ff879ea7267c",
      "4be73fb6aa1949b0b9d33ac0201cb621",
      "f64590f34c3645399b975c2b323359b4",
      "6d395a1a4bab40fd8de589519e534f8f",
      "75c41a9cda9947f9a68b187c09ca28d7",
      "823d640abac54c3ebd18db5d388e1c89",
      "fb516a1a2a96484e97489ffeddb41c04",
      "9d6d210f04f14c68bf187173a4c78311",
      "558c8e4336634e9a8538357834c59e17",
      "1cdc3da329c54ac28be3f1c32d1b147a",
      "8b02d6c12bd74220bcfdf9f217548298",
      "97564839da9d4f338df6cb8f2e4c18f8",
      "bffa464e30eb4665931f2ef03ef76fc5",
      "85b8ff1c194544b6b69107df83cef1cc",
      "afeeb9a936a44f3aae04d65d2297b627",
      "d9c9b7ef3d9949d1a1cd2003fc73d5e3",
      "9abe13cf1bf748059bb7fb40df83a811",
      "893599432dda4d42bfb14b2c92ef9235",
      "99db8839fe0945d7960d8b0ec3727faf",
      "73a6ee206f724b0fa65cae832b35219f",
      "e3c1b7ab67fd48128bf26f24a4a234f1",
      "fa038c6a268c46b295d99a5bda0eb69c",
      "3ec7dfd400cd4620a7dc11c9e61df17a",
      "caf605ceaffa45ff9788b6e93e551f47",
      "8e1fd74e07574694835c98565a71f534",
      "18210d1c8ae144f7a9f70387a5691ad3",
      "97f6dde213fb4a6eb1f4b1de815ee07e",
      "e9abd03dde7041bd85ebaa1e1b9cee6c",
      "d5a7bd838e5341b7a9276049f3a3617e",
      "e36ec96465bd491ab2a4e2cb822cff96",
      "c07438524e854d45b0239d57576f9340",
      "4908fe6842424be6bb4b4a22a9053465",
      "3f892ba7ad584f769ab04937d0286c28",
      "2d932343a12d4a168d4ec9297a1616b3",
      "07e1a61203d24d49bf75610ee0357a9c",
      "ad7881101d2c485398bad279be474549",
      "16d732c5b7914fba8b780d9e5266973f",
      "8206c8e301c84e658b050cf806a9317c",
      "51e3553e8424476d807dfe61f7576892",
      "35ea3cc471e8437ea242bfffe8a72cb7",
      "b0e0d1365e38471c8af9389619e73ad5",
      "8bcac22c43644d4f9abafc9e56f919da",
      "1dbe9f5a62e449d5bdf670a8117cdd70",
      "80bed58df76e46f596e3adabf46cdd5b",
      "a3b5b74ad8164016a19bdb9742b6b36e",
      "9d2ed83d6c3b4e0ead2e56bae1ca82d6",
      "1c8dca7e673849b5bc4368a6f91c0e01",
      "d184c20177ce46a7901713c3205077ef",
      "271b56c7900f456cb63bd29666068c2d",
      "8e7ca59b7dae459a9e21f278efdced19",
      "3a030cb164c74badab29c7aa91a22971",
      "3cdf33f992e94b8aa89268159ea38b5a",
      "91b2af8eb88141038311b33fee9a4f58",
      "946e550cf6314898bb43bd0319847f04",
      "ffd67fe25f514b91b6a64f104b2246c2",
      "85c5acc08f2e4c60980d0f0e7b68640d",
      "08dcfd7089f94360838303bade9727f8",
      "b5e0f8431e7043ac90c852187bfe7a04",
      "fb96398d576c49c5b2688bc82dc23325",
      "57add3f7740a43a089b52b194fdd4b17",
      "b1cf6021afc94ba9ba5297e68dadbdf5",
      "a79a7d0a070d48d39e5fef5efb2870be",
      "fcdacdeed0d24f32af56f8b498ec3c33",
      "9ca1302fd5b241e5b93ac163eb144ea6",
      "24a164a806b44c5b90a01e9d4eb70026",
      "39666a5bbf514310a040fa5497e1f33d",
      "ca64ba2912974e35ac534203e77fb661",
      "4fe1a0464e0f4a038592da007d411203",
      "7edf49b4bd06470189bc73be82b52806",
      "1150383f32db4299a241446a22ba7c22",
      "df36feaeac9f4a15882715aea02463f4",
      "ce10fe7583814ae2b345c3e532e1f302",
      "4defd37e4a4741128cc26b85cff14c02",
      "a0791891c0ed48d987df3ec56e388ea2",
      "0e2e7a22ab81461ea2667129276e7706",
      "1a8f9d77d2a04bfdb10f4d9f414017fb",
      "58ed417908c740bb850893c79580e67c",
      "1d612c2947b642eeb6422bd86668f4b7",
      "da8fe3041ea84554ad65330acb22bb63",
      "a492ef0c9c614f8d94734d30c7f447d8",
      "d7ce6fcbb9d644c5b0ea58eb2b82b80a",
      "d7353a5a6eff4c27b0ca2c1e9e7da56e",
      "b4d5f8112fc04fe99513b9fcec6cf239",
      "3df488bd5fce4ab88fece73ae5cdf67a",
      "37a298b556e14cd8a2f4a6fd122d4df9",
      "75deace76e3d4b62ae6ab54f994db82f",
      "18cb2b641d874f6a9fa743d8f4f7e8a3",
      "0ee9e4c192f6479a90439e3301bae782",
      "536605a314954a5ba5249203d934319e",
      "31106da1d5ac40639584e14f4217d997",
      "79f25583d4904f368c22768697d31275",
      "fe9365760d9041f8b86a4f0e2dde651c",
      "4900bc32c24c4397bbdd0dc900593cb4",
      "16d57c31da974d4bb6411e0342aed65d",
      "b9fb327d3e404fe1b142bdc20054800c",
      "657c7f90d92d4e46b7cd6eb170979345",
      "4ed63acfdcf14a449c025d0e521184c0",
      "23716877cf654b97938216c1a8731b22",
      "3bae307aefb147629d0dbc5daf301f49",
      "58bca51acd3d40bba30e106c87f48a1b",
      "6cd41129878b42fc8e8943f002e88fee",
      "2206e448985e4a32875c7226d67b14de",
      "dfff3ebd3fa8409f86f2512cd341f630",
      "f038cef5dea7474baf394e6f5d2aaad0",
      "d1ad60a1030144cf8edf59dced7382d0",
      "a625318f3b4f481d9840f3bf7dbcacdf",
      "5cf5e240d10348669a6d117d90aaa08f",
      "18e09268a6be48d5a134948efa5e3492",
      "77e9184784034e4a968813e55927f51c",
      "140bf146a9f74630ae6f61b05afe54a7",
      "d6aa3887387b414aaebf508328e06f25"
     ]
    },
    "outputId": "2dc2bdff-ce74-4f53-fdee-92d209d80d24"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "759f338412dd49b390658c7835bcf2a9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af9c9edccd1749cc86d43c2f2e6bfd70"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  ...e-float16/tokenizer.model:  96%|#########6| 4.53MB / 4.70MB            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49d6468db8b44238b6f42e5f44394dc4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  ...Le-float16/tokenizer.json:  75%|#######5  | 25.1MB / 33.4MB            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92bbca6c52f34112b028b53692e4a7b7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aff0bbbed54242d48819f4b808e0f2bc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00004.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rUnsloth: Preparing safetensor model files:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f00f09ee284342c7ba62173e58bddc97"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rUnsloth: Preparing safetensor model files:  25%|\u2588\u2588\u258c       | 1/4 [00:07<00:22,  7.65s/it]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5b38fc16fd934347a150669b021fb2cd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rUnsloth: Preparing safetensor model files:  50%|\u2588\u2588\u2588\u2588\u2588     | 2/4 [00:21<00:22, 11.02s/it]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "706b4290b3c64227b40be09d4907c6e9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rUnsloth: Preparing safetensor model files:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:34<00:12, 12.02s/it]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.66G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd55c5a25bcb4c6baac6f2b138bd4714"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:41<00:00, 10.35s/it]\n",
      "Unsloth: Merging weights into 16bit:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e3b9479c64d04f0080636c892c259ed9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a9d2f5f9ff7a45e7a818f6fa0e1f6280"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  ...0001-of-00004.safetensors:   1%|          | 22.6MB / 3.08GB            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4be73fb6aa1949b0b9d33ac0201cb621"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rUnsloth: Merging weights into 16bit:  25%|\u2588\u2588\u258c       | 1/4 [00:50<02:30, 50.25s/it]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bffa464e30eb4665931f2ef03ef76fc5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "caf605ceaffa45ff9788b6e93e551f47"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  ...0002-of-00004.safetensors:   1%|          | 41.9MB / 4.97GB            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "07e1a61203d24d49bf75610ee0357a9c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rUnsloth: Merging weights into 16bit:  50%|\u2588\u2588\u2588\u2588\u2588     | 2/4 [02:02<02:06, 63.24s/it]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d2ed83d6c3b4e0ead2e56bae1ca82d6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "08dcfd7089f94360838303bade9727f8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  ...0003-of-00004.safetensors:   0%|          | 4.21MB / 4.99GB            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4fe1a0464e0f4a038592da007d411203"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rUnsloth: Merging weights into 16bit:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [03:43<01:20, 80.66s/it]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da8fe3041ea84554ad65330acb22bb63"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "31106da1d5ac40639584e14f4217d997"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  ...0004-of-00004.safetensors:   0%|          | 36.1kB / 2.66GB            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6cd41129878b42fc8e8943f002e88fee"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [04:42<00:00, 70.51s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: Merge process complete. Saved to `/content/chimbiwide/GemmaReLe-float16`\n"
     ]
    }
   ],
   "source": [
    "if True: # Change to True to upload finetune\n",
    "    model.push_to_hub_merged(\n",
    "        \"chimbiwide/GemmaReLe-float16\", tokenizer,\n",
    "        token = \"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCv4vXHd61i7"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now for all models! For now, you can convert easily to `Q8_0, F16 or BF16` precision. `Q4_K_M` for 4bit will come later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqfebeAdT073"
   },
   "outputs": [],
   "source": [
    "if True: # Change to True to save to GGUF\n",
    "    model.save_pretrained_gguf(\n",
    "        \"gemma-3N-finetune-beta\",\n",
    "        quantization_type = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q974YEVPI7JS"
   },
   "source": [
    "Likewise, if you want to instead push to GGUF to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZgcJIhJ0I_es"
   },
   "outputs": [],
   "source": [
    "if False: # Change to True to upload GGUF\n",
    "    model.push_to_hub_gguf(\n",
    "        \"HF_ACCOUNT/gemma-3N-finetune-gguf\",\n",
    "        tokenizer,\n",
    "        quantization_method = \"Q8_0\", # Only Q8_0, BF16, F16 supported\n",
    "        token = \"hf_...\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnz9QOYTMvbH"
   },
   "source": [
    "Now, use the `gemma-3N-finetune.gguf` file or `gemma-3N-finetune-Q4_K_M.gguf` file in llama.cpp.\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "Some other links:\n",
    "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
    "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
    "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
    "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
    "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
    "\n",
    "  Join Discord if you need help + \u2b50\ufe0f <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> \u2b50\ufe0f\n",
    "</div>\n",
    "\n",
    "  This notebook and all Unsloth notebooks are licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}